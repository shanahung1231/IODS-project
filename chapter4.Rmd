---
title: "chapter4.Rmd"
author: "Wenhsuan Hung"
date: "2019/11/22"
output: html_document
---

# Clustering and classification

**1.Explore the structure and the dimensions of the data**

```{r echo = FALSE}
# access the MASS package
library(MASS)
# load the data
data("Boston")
# explore the dataset
str(Boston)
dim(Boston)
```

Boston data (housing values in suburbs of Boston) has 506 observations and 14 variables. Variables include "crim"(per capita crime rate by town), "zn"(proportion of residential land zoned for lots over 25,000 sq.ft), "rm"(average number of rooms per dwelling). Through analyzing the data, we hope to understand what are the features which affect housing price. 


**2.Show a graphical overview of the data and show summaries of the variables in the data.**

```{r echo = FALSE}
library(MASS) ; library(corrplot) ; library(dplyr); library(ggplot2); library(tidyr)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(2) 
# print the correlation matrix
cor_matrix
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

summary(Boston)
```

The correlation matrix shows the relationship between two variables. If the dot is blue, it means that the two variables are positively correlated; if the dot is red, it means that the two variables are negatively correlated. As the color gets darker (blue or red), it means that the two variables have a stronger correlation. The dot is white or almost white if the two variables have weak correlation or no correlation. For example, "rad" and "tax" have high positive correlation; "lstat" and "medv", "age" and "dis" have negative correlation. According to the summary, I see the minimum, maximum, mean, quartile value of each variable. For example, the average number of rooms per dwelling is 6.285, and the average crime rate 3.613.

**3.Standardize the dataset and print out summaries of the scaled data.**

```{r echo = FALSE}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```
After scaling the data, the mean of each variable is 0, and the difference between each variable become smaller. 

**4.Create a categorical variable of the crime rate in the Boston dataset, and drop the old crime rate variable from the dataset.**

```{r echo = FALSE}
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

**5.Divide the dataset to train and test sets, so that 80% of the data belongs to the train set, and fit the linear discriminant analysis on the train set.**

```{r echo = FALSE}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```



```{r echo = FALSE}
# linear discriminant analysis
lda.fit <- lda(crime~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 1)
```

**6.Save the crime categories from the test set and then remove the categorical crime variable from the test dataset, then predict the classes with the LDA model on the test data.**

```{r echo = FALSE}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes , predicted = lda.pred$class)
```

The result of the cross tabulation shows the relation between the prediction and the correct answer. For example, there are 21 when the preidction is low and the correct answer is low, and there are 5 when the preidction is low and the correct answer is med_low.

**7.Reload the Boston dataset and standardize the dataset, and calculate the distances between the observations. **

```{r echo = FALSE}
data('Boston')
boston_scaled_new <- scale(Boston)
# euclidean distance matrix
dist_eu <- dist(boston_scaled_new)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled_new, method = 'manhattan')

# look at the summary of the distances
summary(dist_man)

```

I calculate the distance with Euclidean distance measure and Manhattan distance measure. According to the result, the value from Manhattan distance measure is larger than Euclidean distance measure.(e.g. the median of Euclidean distance measure is 4.8241, and the median of Manhattan distance measure is 13.5488.)

**8.Run k-means algorithm on the dataset.Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters and interpret the results.**

```{r echo = FALSE}
# k-means clustering
km <-kmeans(boston_scaled_new, centers = 3)

# plot the Boston dataset with clusters
pairs(Boston[6:10], col = km$cluster)

set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled_new, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(boston_scaled_new, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled_new, col = km$cluster)
```

The visualization of clusters show the distribution of clusters of different variables. Since we set 3 clusters, there are three colors in each plots, representing each cluster. The line chart shows that when the cluster is around 1.25, the total within sum of square is the highest.

