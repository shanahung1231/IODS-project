---
title: "chapter3.Rmd"
author: "Wenhsuan Hung"
date: "2019/11/16"
output: html_document
---

# Logistic regression

```{r echo = FALSE}
# access dplyr and ggplot2
library(dplyr); library(ggplot2); library(tidyr)
#read the joined student alcohol consumption data
alc <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt", sep = ",",header=TRUE)
str(alc)
```
This is ａdata discussing about students' alchohol consumption.There are 382 observations and 35 variables. Variables include student's, sex, age,family size,alcohol consumption, parents'education status, job, and so on. Through the analysis, I want to study the relationships between high/low alcohol consumption and some of the other variables in the data.

I assume that "studytime"(weekly study time), "failures"(number of past class failures), "goout"(frequency of going out with friends), "freetime"(free time after school), are important variables which have strong relationship with the consumption of alchohol. My hypothesis is that students who have less study time per week, fail more classes, go out with friends more often, have more free time after school consume more alchohol.

**1.Numerically and graphically explore the distributions**
```{r echo = FALSE}
# produce summary statistics by group
alc %>% group_by(sex, high_use) %>% summarise(count = n(), mean_study_time = mean(studytime))

alc %>% group_by(sex, high_use) %>% summarise(count = n(), mean_failures = mean(failures))

# initialize a plot of high_use and G3
g1 <- ggplot(alc, aes(x = high_use, y = goout, col = sex))

# define the plot as a boxplot and draw it
g1 + geom_boxplot() + ggtitle("Student's go out frequency by alcohol consumption and sex")

# initialise a plot of high_use and absences
g2 <- ggplot(alc, aes(x = high_use, y = freetime, col = sex))

# define the plot as a boxplot and draw it
g2 + geom_boxplot() + ggtitle("Student freetime by alcohol consumption and sex")
```

According to the summary statistics of study time group by sex and high_use, female who has shorter study time tend to consume more alchohol (more than 2 times per week), and those who studies longer tend not to consume that much alchohol(less than 2 times per week). The phenomenon is same for male. The result corresponds to my assumption. 

Through the summary statistics of failures group by sex and high_use, female who failed more classes in the past tend to consume more alchohol, and those who failed less classes tend not to consume that much alchohol. This also happens at male. The result corresponds to my assumption.

The boxplots shows that for variable "goout", female who consumes more alchohol  goes out more. Male also has the same same situation. The result corresponds to what i assumed. And for variable "freetime", female who consumes more alchohol has more freetime after school; however, the phenomenon is not too significant for male. The result is similiar to my assumption, but doesn't exactly match. 

**2.Use logistic regression to statistically explore the relationship between your chosen variables and the binary high/low alcohol consumption variable as the target variable.**

I choose "studytime", "failures", "goout", "freetime" as four X variables, and fit them into the model which Y is "high_use" (high/low alcohol consumption). I also separate the data into male and female, in order to dig more into the data. Below are what I found from the model.

```{r echo = FALSE}
# find the model with glm()
m <- glm(high_use ~ studytime + failures + goout + freetime, data = alc, family = "binomial")

# print out a summary of the model
summary(m)

# print out the coefficients of the model
coef(m)

# compute odds ratios (OR)
OR <- coef(m) %>% exp
# compute confidence intervals (CI)
CI <- confint(m) %>% exp

# print out the odds ratios with their confidence intervals
cbind(OR, CI)
```

The summary shows that standard error for "studytime", "failures", "goout", "freetime" are  0.79, 0.79, 0.74, 0.75, which are comparatively small. Smaller standard error indicates that sample mean and the population mean is more similar, which means that sample data has a stronger explanatory power to the population. The coefficient for "studytime", "failures", "goout", "freetime" are -2.94, -2.18, -1.67, -2.3, which means that these variables are all highly correlated with Y("high_use"). 

The odds ratio of "studytime" is 0.654（less than 1), "failures" is 1.34（higher than 1), "goout" is 2.114（higher than 1), "freetime" is 1.164（higher than 1), which means that "failures", "goout", "freetime" is positively associated with "high_use". According to the confidence intervals, 

According to the above result, I now figure out that "failures", "goout", "freetime" are important variables, which are correlated with high/low alcohol consumption. Since the odds ratio of "studytime" is less than one, it isn't a factor which has high relationship with high/low alcohol consumption.

**3.Using the variables which has statistical relationship with high/low alcohol consumption to explore the predictive power of you model. **

```{r echo = FALSE}
m_new <- glm(high_use ~ failures + goout + freetime, data = alc, family = "binomial")

# predict() the probability of high_use
probabilities <- predict(m_new, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability = probabilities)

# use the probabilities to make a prediction of high_use
alc <- mutate(alc, prediction = probabilities > 0.5)

# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = probabilities > 0.5)
```

Since I found that "studytime" isn't an important factor, I remove this variable, and predict a model. According to the confusion matrix, we can calculate the precision is 248/(248+76) = 0.77, and the recall is 248/ (248+22) = 0.92. This means that the model has high recall, low precision, which means that most of the positive examples are correctly recognized, but there are a lot of false positives.The average number of wrong predictions in training data is 0.2565445, and the average number of wrong predictions in the cross validation 0.2486911. This means that the error of the model is quite high(around 25%).

